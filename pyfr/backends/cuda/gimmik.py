from gimmik import CUDAMatMul
import numpy as np

from pyfr.backends.base import NotSuitableError
from pyfr.backends.cuda.provider import CUDAKernel, CUDAKernelProvider


class CUDAGiMMiKKernels(CUDAKernelProvider):
    def __init__(self, backend):
        super().__init__(backend)

        # Maximum number of kernels to consider
        self.nkerns = backend.cfg.getint('backend-cuda', 'gimmik-nkerns', 8)

        # Number of benchmarking runs
        self.nbench = backend.cfg.getint('backend-cuda', 'gimmik-nbench', 5)

        # Kernel cache
        self._mul_kerns = {}

    def mul(self, a, b, out, alpha=1.0, beta=0.0):
        # Ensure the matrices are compatible
        if a.nrow != out.nrow or a.ncol != b.nrow or b.ncol != out.ncol:
            raise ValueError('Incompatible matrices for out = a*b')

        # Check that A is constant
        if 'const' not in a.tags:
            raise NotSuitableError('GiMMiK requires a constant a matrix')

        # Fetch the matrix and tally up the number of non-zeros
        arr = a.get()
        nnz, nuq = np.count_nonzero(arr), len(np.unique(np.abs(arr)))

        # Check that A is suitable
        if nuq > 28 and nnz / arr.size > 0.15:
            raise NotSuitableError('Matrix is inappropriate for GiMMiK')

        # Dimensions
        ldb, ldc = b.leaddim, out.leaddim

        # Alignment
        if 'align' in b.tags and 'align' in out.tags:
            aligne = self.backend.alignb // b.itemsize
        else:
            aligne = None

        # Cache key
        ckey = (a.mid, alpha, beta, aligne, ldb, ldc)

        # Check the kernel cache
        try:
            kern, grid, block, dt = self._mul_kerns[ckey]
        except KeyError:
            kname = f'gimmik_mm_{arr.shape[0]}x{arr.shape[1]}'
            kdata = None
            best_kern = None

            # Save a copy of the contents of the output matrix
            out_np = getattr(out, 'parent', out).get()

            mm = CUDAMatMul(alpha*arr, beta=beta, aligne=aligne, n=b.ncol,
                            ldb=ldb, ldc=ldc)
            kgen = mm.kernels(
                a.dtype, kname=kname,
                compute_capability=self.backend.cuda.compute_capability()
            )

            # Benchmark the sequence of kernels generated by GiMMiK
            try:
                for i in range(self.nkerns):
                    src, meta = kgen.send(kdata)
                    kern = self._build_kernel(kname, src, 'PP')

                    # If the kernel is spilling then request more L1
                    if kern.local_mem and not kern.shared_mem:
                        kern.set_shared_size(carveout=0)

                    # Set the parameters
                    params = kern.make_params(meta['grid'], meta['block'])
                    params.set_args(b, out)

                    # Obtain the runtime
                    dt = self._benchmark(
                        lambda stream: kern.exec_async(stream, params),
                        nbench=self.nbench
                    )

                    if best_kern is None or dt < best_kern[-1]:
                        best_kern = kern, meta['grid'], meta['block'], dt

                    kdata = {
                        'runtime': dt,
                        'registers': kern.nreg,
                        'local_mem': kern.local_mem
                    }
            except StopIteration:
                pass

            # Restore the output matrix
            getattr(out, 'parent', out).set(out_np)

            # Update the cache
            self._mul_kerns[ckey] = kern, grid, block, dt = best_kern

        # Set the parameters
        params = kern.make_params(grid, block)
        params.set_args(b, out)

        class MulKernel(CUDAKernel):
            def add_to_graph(self, graph, deps):
                return graph.graph.add_kernel(params, deps)

            def run(self, stream):
                kern.exec_async(stream, params)

        return MulKernel(mats=[b, out], dt=dt)
